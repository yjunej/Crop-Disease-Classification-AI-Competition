{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 작물 병해 분류 AI 경진대회 Private 2위, Private Score: 0.99848,  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yjune팀, 사용모델: SE-ResNeXt101-32x4d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대회 개요\n",
    "Task: Multi-class image classification\\\n",
    "평가 산식: f1-macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험과정\n",
    "* train.csv 데이터를 5 fold로 나누어 5개의 모델을 학습하고 앙상블 적용\n",
    "* test 데이터에서 각 클래스에 대한 softmax를 거친 예측값이 0.85 이상일 경우 pseudo-labeling 하여 train data에 추가\n",
    "* pseudo-labeling 데이터가 추가된 최종데이터를 5fold로 나누고 학습하고 5개의 모델을 앙상블\n",
    "* 최종 모델 완성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning.accelerators import accelerator\n",
    "import torch\n",
    "import torchvision\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "from pytorch_lightning import seed_everything\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from config import Config\n",
    "# from models.model import FDModule\n",
    "# from dataset import FDDataModule\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# from config import Config\n",
    "import timm\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchmetrics import F1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# from utils.loss import FocalLoss\n",
    "# from utils.utils import mixup_data, mixup_criterion\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision.transforms.transforms import ColorJitter, RandomCrop, RandomHorizontalFlip\n",
    "\n",
    "from config import Config\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import albumentations.pytorch as Ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     uid              img_path    disease  disease_code\n",
      "0  10000  train_imgs/10000.jpg    시설포도노균병             1\n",
      "1  10001  train_imgs/10001.jpg    시설포도노균병             1\n",
      "2  10002  train_imgs/10002.jpg  시설포도노균병반응             2\n",
      "3  10003  train_imgs/10003.jpg        축과병             4\n",
      "4  10004  train_imgs/10004.jpg    시설포도노균병             1\n",
      "(250, 4)\n",
      "(4750, 2)\n",
      "0    106\n",
      "1     46\n",
      "2     30\n",
      "3     29\n",
      "4     17\n",
      "5     12\n",
      "6     10\n",
      "Name: disease_code, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0    1745\\n1     846\\n2     640\\n3     611\\n4     399\\n5     303\\n6     206\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = 'fd_data'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
    "\n",
    "print(train_df.head())\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "\n",
    "print(train_df['disease_code'].value_counts())\n",
    "\"\"\"\n",
    "0    106\n",
    "1     46\n",
    "2     30\n",
    "3     29\n",
    "4     17\n",
    "5     12\n",
    "6     10\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "0    1745\n",
    "1     846\n",
    "2     640\n",
    "3     611\n",
    "4     399\n",
    "5     303\n",
    "6     206\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train data가 매우 적고, imblanced class\n",
    "* f1-macro 점수 향상을 위해서는 데이터가 적은 class에서도 좋은 점수를 얻어야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config & pre-defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixup augmentation을 위한 코드입니다.\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"Mixup for binary classification\n",
    "    Args:\n",
    "        x (torch.Tensor): batch of inputs\n",
    "        y (torch.Tensor): batch of binary labels \n",
    "        alpha (float, optional): Defaults to 1.0.\n",
    "    Returns:\n",
    "        mixed_x, y_a, y_b, lam\n",
    "    \"\"\"\n",
    "\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam, weight):\n",
    "    return lam * criterion(pred, y_a, weight=weight) + (1 - lam) * criterion(pred, y_b,weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    exp = 'exp_1' # exp_1: 기존 train data만 사용, exp_2: pseudo labeling 추가한 최종 train data 사용\n",
    "    phase = 'train' # train or test\n",
    "    data_dir = 'fd_data' \n",
    "    model_name = 'gluon_seresnext101_32x4d' # timm의 ImageNet pretrained 모델\n",
    "    fold_num = 5 # k-fold \n",
    "    batch_size = 64 # \n",
    "    num_workers = 4\n",
    "    seed = 555\n",
    "    tta = True # Test Time Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch lightning Dataset \n",
    "\n",
    "class FDDataset(Dataset):\n",
    "    def __init__(self, cfg:Config, df:pd.DataFrame, aug:bool = True):\n",
    "        super(FDDataset, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.aug = aug\n",
    "        if self.aug:\n",
    "            self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomAffine(\n",
    "                degrees=(-90,90),\n",
    "                translate=(0.2, 0.2),\n",
    "                scale=(0.8, 1.2), shear=15\n",
    "                ),\n",
    "            ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                transforms.Resize((256,256)),\n",
    "            ]\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.cfg.data_dir, self.df.loc[idx, 'img_path'])\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = self.transform(img)\n",
    "        if self.cfg.phase == 'test':\n",
    "            return img, self.df.loc[idx, 'uid']\n",
    "        label = self.df.loc[idx, 'disease_code']\n",
    "        return img, label\n",
    "        \n",
    "class FDDataModule(LightningDataModule):\n",
    "    def __init__(self, cfg:Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.test_df = pd.read_csv(os.path.join(cfg.data_dir, 'test.csv'))\n",
    "        self.train_df = pd.read_csv(os.path.join(cfg.data_dir, 'train.csv'))\n",
    "        self.fold_num = 0\n",
    "        self._split_kfold()\n",
    "\n",
    "    def set_fold_num(self, fold_num):\n",
    "        self.fold_num = fold_num\n",
    "\n",
    "    def get_class_weight(self):\n",
    "        return 1 / self.train_df['disease_code'].value_counts().sort_index().values\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage != 'test':\n",
    "            print(f'FOLD NUM:{self.fold_num}')\n",
    "            train_df = self.train_df[\n",
    "                self.train_df[\"kfold\"] != self.fold_num\n",
    "            ].reset_index(drop=True)\n",
    "            val_df = self.train_df[\n",
    "                self.train_df[\"kfold\"] == self.fold_num\n",
    "            ].reset_index(drop=True)\n",
    "\n",
    "            self.train = FDDataset(self.cfg, train_df, aug=True)\n",
    "            self.val = FDDataset(self.cfg, val_df, aug=False)\n",
    "            self.train_fold_df = self.train_df\n",
    "\n",
    "        if stage == 'test':\n",
    "            self.test = FDDataset(self.cfg, self.test_df, aug=False)\n",
    "\n",
    "    def _split_kfold(self):\n",
    "        skf = StratifiedKFold(\n",
    "            n_splits=Config.fold_num, shuffle=True, random_state=Config.seed\n",
    "        )\n",
    "        # (train_idx, val_idx)\n",
    "        for n, (_, val_index) in enumerate(\n",
    "            skf.split(\n",
    "                X=self.train_df,\n",
    "                y=self.train_df['disease_code']\n",
    "            )\n",
    "        ):  # if valid index, record fold num in 'kfold' column\n",
    "            self.train_df.loc[val_index, \"kfold\"] = int(n)\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train,\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val,\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test,\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch lightning Module\n",
    "\n",
    "class FDModel(nn.Module):\n",
    "    def __init__(self, cfg:Config):\n",
    "        super(FDModel, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.cnn = timm.create_model( # timm ImageNet pre-trained 모델 load\n",
    "            cfg.model_name,\n",
    "            pretrained=True,\n",
    "            num_classes = 7,\n",
    "            in_chans = 3\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        return out\n",
    "\n",
    "class FDModule(LightningModule):\n",
    "    def __init__(self, cfg:Config, class_weight=None):\n",
    "        super().__init__()\n",
    "        self.model = FDModel(cfg)\n",
    "        self.val_metric = F1(num_classes=7, average=\"macro\").cuda()\n",
    "        self.train_metric =  F1(num_classes=7, average=\"macro\").cuda()\n",
    "        self.lr = 1e-3\n",
    "        self.class_weight = class_weight\n",
    "        self.cfg = cfg\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        ## TTA에 사용할 augmentation\n",
    "        self.horizontalflip = transforms.RandomHorizontalFlip(p=1)\n",
    "        self.verticalflip = transforms.RandomVerticalFlip(p=1)\n",
    "        self.rotation_left = transforms.RandomRotation(degrees=(-90,-90))\n",
    "        self.rotation_right = transforms.RandomRotation(degrees=(90,90))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        if batch_idx % 4 == 0: # 4 step 주기로 mixup 사용\n",
    "            mixed_x, y_a, y_b, lam = mixup_data(x, y)\n",
    "            logits = self(mixed_x)\n",
    "            loss = mixup_criterion(F.cross_entropy, logits, y_a, y_b, lam, torch.Tensor(self.class_weight).cuda())\n",
    "            self.log_dict({'mixup_loss':loss})\n",
    "            return loss\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y.long(), weight= torch.Tensor(self.class_weight).cuda())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        micro_acc = accuracy(preds, y)\n",
    "        f1_score = self.train_metric(preds, y)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_loss\": loss,\n",
    "                \"train_acc\": micro_acc,\n",
    "                \"train_f1_macro\": f1_score\n",
    "            },\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True\n",
    "\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y.long(), weight= torch.Tensor(self.class_weight).cuda())\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        micro_acc = accuracy(preds, y)\n",
    "\n",
    "        f1_score = self.val_metric(preds, y)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"val_loss\": loss,\n",
    "                \"val_acc\": micro_acc,\n",
    "                \"val_f1_macro\": f1_score                \n",
    "            },\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True\n",
    "        )\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        if self.cfg.tta:\n",
    "            return self.tta(batch,batch_idx)\n",
    "        x, uid = batch\n",
    "        logits = self(x)\n",
    "        prob = self.softmax(logits)\n",
    "        preds = prob\n",
    "\n",
    "        return preds, uid\n",
    "\n",
    "    def tta(self, batch, batch_idx):\n",
    "        x, uid = batch\n",
    "        _normal = self.softmax(self(x))\n",
    "        _h_flip = self.softmax(self(self.horizontalflip(x)))\n",
    "        _v_flip = self.softmax(self(self.verticalflip(x)))\n",
    "        _l_rotate = self.softmax(self(self.rotation_left(x)))\n",
    "        _r_rotate = self.softmax(self(self.rotation_right(x)))\n",
    "        preds = (_normal + _h_flip + _v_flip + _l_rotate + _r_rotate) / 5\n",
    "        return preds, uid  \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        results = self.all_gather(outputs)\n",
    "        \n",
    "        # class 별 confidence 저장하는 dataframe\n",
    "        df = pd.DataFrame(range(20000,24750),columns=['uid'])\n",
    "        \n",
    "        df['prob_0'] = -100.0\n",
    "        df['prob_1'] = -100.0\n",
    "        df['prob_2'] = -100.0\n",
    "        df['prob_3'] = -100.0\n",
    "        df['prob_4'] = -100.0\n",
    "        df['prob_5'] = -100.0\n",
    "        df['prob_6'] = -100.0\n",
    "\n",
    "        \n",
    "        df = df.set_index('uid')\n",
    "        for p, u in results:\n",
    "            prob = p.reshape(-1,7).cpu().numpy()\n",
    "            u = u.reshape(-1).cpu().numpy()\n",
    "            for pp, uu in zip(prob,u):\n",
    "                df.loc[uu] = pp\n",
    "        df.to_csv(f'result_{self.cfg.exp}.csv')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=30, verbose=True)\n",
    "        \n",
    "        if self.cfg.exp == 'exp_2':\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=1)\n",
    "            return (\n",
    "            {\n",
    "                \"optimizer\":optimizer,\n",
    "                \"lr_scheduler\": {\"scheduler\":scheduler, \"monitor\":\"val_loss\", \"interval\":\"epoch\"},\n",
    "            },\n",
    "            )\n",
    "        \n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 555\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning.accelerators import accelerator\n",
    "\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "from pytorch_lightning import seed_everything\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "def train(cfg: Config, fold_num):\n",
    "    seed_everything(Config.seed)\n",
    "    fd_data_module = FDDataModule(cfg)\n",
    "    # fd_data_module.setup(stage='test')\n",
    "    \n",
    "    #! TRAIN\n",
    "    fd_data_module.set_fold_num(fold_num)\n",
    "    fd_data_module.setup()\n",
    "    class_weight = fd_data_module.get_class_weight()\n",
    "\n",
    "    if cfg.phase=='test':\n",
    "        fd_module = FDModule(cfg, class_weight=None).load_from_checkpoint(cfg.ckpt,\n",
    "         cfg=Config)\n",
    "    else:\n",
    "        fd_module = FDModule(cfg, class_weight=class_weight)\n",
    "    \n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(monitor='val_loss', save_top_k=3, dirpath=f'results/{cfg.exp}/{fd_data_module.fold_num}',\n",
    "    filename=\"{epoch:02d}-{val_loss:.6f}-{val_acc:.4f}-{val_f1_macro}.pth\", mode='min')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=200, verbose=True, mode='min') # for pseudo labeling\n",
    "    # early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=True, mode='min') # for full train\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=\"0\",\n",
    "        accelerator='dp',\n",
    "        num_nodes=1,\n",
    "        deterministic=True,\n",
    "        check_val_every_n_epoch=1,\n",
    "        callbacks = [model_checkpoint, early_stopping],\n",
    "        precision=16,\n",
    "        log_every_n_steps=4,\n",
    "        # 500 for train\n",
    "        # 500 fix\n",
    "        max_epochs = 500,\n",
    "        auto_lr_find=True,\n",
    "        plugins=DDPPlugin(find_unused_parameters=False),\n",
    "    )\n",
    "    \n",
    "    if cfg.phase == 'train':\n",
    "        trainer.fit(fd_module, fd_data_module)\n",
    "    else:\n",
    "        trainer.test(fd_module, fd_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold 0 학습 및 체크포인트 저장\n",
    "train(Config, 0)\n",
    "# Fold 1 학습 및 체크포인트 저장\n",
    "train(Config, 1)\n",
    "# Fold 2 학습 및 체크포인트 저장\n",
    "train(Config, 2)\n",
    "# Fold 3 학습 및 체크포인트 저장\n",
    "train(Config, 3)\n",
    "# Fold 4 학습 및 체크포인트 저장\n",
    "train(Config, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(cfg: Config):\n",
    "    \n",
    "    fd_data_module = FDDataModule(cfg)\n",
    "    fd_data_module.setup(stage='test')\n",
    "    fd_module = FDModule(cfg, class_weight=None).load_from_checkpoint(cfg.ckpt, cfg=Config)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=\"0\",\n",
    "        accelerator='dp',\n",
    "        num_nodes=1,\n",
    "        deterministic=True,\n",
    "        check_val_every_n_epoch=1,\n",
    "        callbacks = [model_checkpoint, early_stopping],\n",
    "        precision=16,\n",
    "        log_every_n_steps=4,\n",
    "        max_epochs = 500,\n",
    "        auto_lr_find=True,\n",
    "        plugins=DDPPlugin(find_unused_parameters=False),\n",
    "    )\n",
    "    trainer.test(fd_module, fd_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.Config"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.phase = 'test'\n",
    "\n",
    "# Fold 0 모델 \n",
    "Config.exp = 'exp_1_fold_0'\n",
    "Config.ckpt = glob('results/exp_1/0/*')[0] \n",
    "test(Config)\n",
    "\n",
    "# Fold 1 모델 \n",
    "Config.exp = 'exp_1_fold_1'\n",
    "Config.ckpt = glob('results/exp_1/1/*')[0] \n",
    "test(Config)\n",
    "\n",
    "# Fold 2 모델 \n",
    "Config.exp = 'exp_1_fold_2'\n",
    "Config.ckpt = glob('results/exp_1/2/*')[0] \n",
    "test(Config)\n",
    "\n",
    "# Fold 3 모델 \n",
    "Config.exp = 'exp_1_fold_3'\n",
    "Config.ckpt = glob('results/exp_1/3/*')[0] \n",
    "test(Config)\n",
    "\n",
    "# Fold 4 모델 \n",
    "Config.exp = 'exp_1_fold_4'\n",
    "Config.ckpt = glob('results/exp_1/4/*')[0] \n",
    "test(Config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_0 = pd.read_csv('result_exp_1_fold_0.csv')\n",
    "fold_1 = pd.read_csv('result_exp_1_fold_1.csv')\n",
    "fold_2 = pd.read_csv('result_exp_1_fold_2.csv')\n",
    "fold_3 = pd.read_csv('result_exp_1_fold_3.csv')\n",
    "fold_4 = pd.read_csv('result_exp_1_fold_4.csv')\n",
    "\n",
    "df = pd.concat([fold_0,fold_1,fold_2,fold_3,fold_4])\n",
    "df = df.groupby('uid').mean()\n",
    "\n",
    "df['_max'] = df.max(axis=1)\n",
    "df['disease_code'] = df.idxmax(axis=1).str[-1].astype(int)\n",
    "\n",
    "pseudo_label_df = df[df['_max'] > 0.85][['disease_code']] \n",
    "pseudo_label_df['img_path'] = 'test_imgs/'+pseudo_label_df.index.astype(str)+'.jpg'\n",
    "pseudo_label_df = pseudo_label_df.reset_index()[['uid','img_path','disease_code']]\n",
    "\n",
    "org_train = pd.read_csv('fd_data/train.csv')\n",
    "full_train_df = pd.concat([org_train[['uid','img_path','disease_code']], pseudo_label_df],axis=0)\n",
    "full_train_df.to_csv('full_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Override Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch lightning Dataset \n",
    "\n",
    "class FDDataset(Dataset):\n",
    "    def __init__(self, cfg:Config, df:pd.DataFrame, aug:bool = True):\n",
    "        super(FDDataset, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.aug = aug\n",
    "        if self.aug:\n",
    "            self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomAffine(\n",
    "                degrees=(-90,90),\n",
    "                translate=(0.2, 0.2),\n",
    "                scale=(0.8, 1.2), shear=15\n",
    "                ),\n",
    "            ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                transforms.Resize((256,256)),\n",
    "            ]\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.cfg.data_dir, self.df.loc[idx, 'img_path'])\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = self.transform(img)\n",
    "        if self.cfg.phase == 'test':\n",
    "            return img, self.df.loc[idx, 'uid']\n",
    "        label = self.df.loc[idx, 'disease_code']\n",
    "        return img, label\n",
    "        \n",
    "class FDDataModule(LightningDataModule):\n",
    "    def __init__(self, cfg:Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.test_df = pd.read_csv(os.path.join(cfg.data_dir, 'test.csv'))\n",
    "        self.train_df = pd.read_csv(os.path.join(cfg.data_dir, 'full_train.csv'))\n",
    "        self.fold_num = 0\n",
    "        self._split_kfold()\n",
    "\n",
    "    def set_fold_num(self, fold_num):\n",
    "        self.fold_num = fold_num\n",
    "\n",
    "    def get_class_weight(self):\n",
    "        return 1 / self.train_df['disease_code'].value_counts().sort_index().values\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage != 'test':\n",
    "            print(f'FOLD NUM:{self.fold_num}')\n",
    "            train_df = self.train_df[\n",
    "                self.train_df[\"kfold\"] != self.fold_num\n",
    "            ].reset_index(drop=True)\n",
    "            val_df = self.train_df[\n",
    "                self.train_df[\"kfold\"] == self.fold_num\n",
    "            ].reset_index(drop=True)\n",
    "\n",
    "            self.train = FDDataset(self.cfg, train_df, aug=True)\n",
    "            self.val = FDDataset(self.cfg, val_df, aug=False)\n",
    "            self.train_fold_df = self.train_df\n",
    "\n",
    "        if stage == 'test':\n",
    "            self.test = FDDataset(self.cfg, self.test_df, aug=False)\n",
    "\n",
    "    def _split_kfold(self):\n",
    "        skf = StratifiedKFold(\n",
    "            n_splits=Config.fold_num, shuffle=True, random_state=Config.seed\n",
    "        )\n",
    "        # (train_idx, val_idx)\n",
    "        for n, (_, val_index) in enumerate(\n",
    "            skf.split(\n",
    "                X=self.train_df,\n",
    "                y=self.train_df['disease_code']\n",
    "            )\n",
    "        ):  # if valid index, record fold num in 'kfold' column\n",
    "            self.train_df.loc[val_index, \"kfold\"] = int(n)\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train,\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val,\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test,\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.exp = 'exp_2'\n",
    "\n",
    "# Fold 0 학습 및 체크포인트 저장\n",
    "train(Config, 0)\n",
    "# Fold 1 학습 및 체크포인트 저장\n",
    "train(Config, 1)\n",
    "# Fold 2 학습 및 체크포인트 저장\n",
    "train(Config, 2)\n",
    "# Fold 3 학습 및 체크포인트 저장\n",
    "train(Config, 3)\n",
    "# Fold 4 학습 및 체크포인트 저장\n",
    "train(Config, 4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}